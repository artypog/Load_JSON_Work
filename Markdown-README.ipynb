{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184aec12",
   "metadata": {},
   "source": [
    "![](https://ludi.by/wp-content/uploads/2023/01/desktop_69d8542e39243e90f0bff7de2014dbbb91cdb65a.png)\n",
    "\n",
    "# Анализ обработки данных «СберАвтоподписка»\n",
    "\n",
    "## Требования к работе\n",
    "Корректность создания объектов в БД:\n",
    "1. Корректность создания объектов в БД:\n",
    "- Корректная типизация данных.\n",
    "- Настройка первичных и внешних ключей.\n",
    "\n",
    "2. Корректность добавления новых данных в БД:\n",
    "- Обработка и типизация новых данных.\n",
    "- Имеется механизм разрешения конфликтов при записи.\n",
    "\n",
    "3. Реализация Airflow-пайплайна:\n",
    "- Наличие и корректность всех зависимостей между DAG’ами / task’ами\n",
    "- Качество реализации (чистота кода, идемпотентность, использование переменных, наличие обращений в БД вне task’ов).\n",
    "\n",
    "## Описание данных\n",
    "Данные из Google Analytics (last-click attribution model) по сайту «СберАвтоподписка».\n",
    "\n",
    "### GA_Sessions \n",
    "\n",
    "Одна строка = один визит на сайт.\n",
    "Описание атрибутов:\n",
    "\u0019 session_id — ID визита;\n",
    "\u0019 client_id — ID посетителя;\n",
    "\u0019 visit_date — дата визита;\n",
    "\u0019 visit_time — время визита;\n",
    "\u0019 visit_number — порядковый номер визита клиента;\n",
    "\u0019 utm_source — канал привлечения;\n",
    "\u0019 utm_medium — тип привлечения;\n",
    "\u0019 utm_campaign — рекламная кампания;\n",
    "\u0019 utm_keyword — ключевое слово;\n",
    "\u0019 device_category — тип устройства;\n",
    "\u0019 device_os — ОС устройства;\n",
    "\u0019 device_brand — марка устройства;\n",
    "\u0019 device_model — модель устройства;\n",
    "\u0019 device_screen_resolution — разрешение экрана;\n",
    "\u0019 device_brand — марка устройства;\n",
    "\u0019 device_model — модель устройства;\n",
    "\u0019 device_screen_resolution — разрешение экрана;\n",
    "\u0019 device_browser — браузер;\n",
    "\u0019 geo_country — страна;\n",
    "\u0019 geo_city — город.\n",
    "\n",
    "### GA_Hits\n",
    "\n",
    "Одна строка = одно событие в рамках одного визита на сайт.\n",
    "Описание атрибутов:\n",
    "\u0019 session_id — ID визита;\n",
    "\u0019 hit_date — дата события;\n",
    "\u0019 hit_time — время события;\n",
    "\u0019 hit_number — порядковый номер события в рамках сессии;\n",
    "\u0019 hit_type — тип события;\n",
    "\u0019 hit_referer — источник события;\n",
    "\u0019 hit_page_path — страница события;\n",
    "\u0019 event_category — тип действия;\n",
    "\u0019 event_action — действие;\n",
    "\u0019 event_label — тег действия;\n",
    "\u0019 event_value — значение результата действия.\n",
    "\n",
    "## Что нужно сделать\n",
    "### Проведите подготовительную работу:\n",
    "-  Прочитайте предоставленный датасет.\n",
    "-  Ознакомьтесь с описаниями представленных атрибутов.\n",
    "-  Оцените полноту и чистоту данных. Попытайтесь понять, что стоит за этими данными в реальном мире. Приведите данные в удобный/нормальный вид для дальнейшей работы.\n",
    "    \n",
    "### Проведите разведочный анализ данных:\n",
    "-  Проведите базовую чистку (дубликаты, пустые значения, типизация данных, ненужные атрибуты).\n",
    "-  Посмотрите на распределение ключевых атрибутов, их отношения.\n",
    "    \n",
    "###  Выполните задание согласно вашей специализации:\n",
    "-  Настройте и запустите локальную БД, подходящую для хранения и исполнения запросов к данным в предоставленном датасете.\n",
    "-  Создайте объекты в БД для хранения данных исходного файла.\n",
    "-  Обработайте и поместите в БД данные из предоставленного основного датасета.\n",
    "-  Настройте пайплайн сбора, обработки и записи в БД новых .json-файлов. \n",
    "\n",
    "## Подготовительные работы\n",
    "\n",
    "### Распаковка архива\n",
    "Архив необходимо скачать и распаковать его в домашний каталог пользователя. Домашний каталог пользователя в командной строке обозначается символом \"~\". В результате распаковки архива в Вашем домашнем каталоге должен появиться каталог \"airflow_hw\". \n",
    "\n",
    "### Настройка окружения\n",
    "В корневом каталоге  \"~/airflow_hw\" создайте файл .env c содержимым:\n",
    "- dbname=<название схемы>\n",
    "- user=<имя пользователя для доступа к БД>\n",
    "- password=<пароль пользователя для доступа к БД>\n",
    "- host=<Имя хоста или IP адрес сервера с БД>\n",
    "\n",
    "### Установка сервера PostGreSql\n",
    "Установка сервера выполняется согласно документации по установке, с учетом операционной системы. Если имеется готовый, уже развернутый сервер, то можно выполнить просто его настройку:\n",
    "1. Создать рабочую схему, например dbtest;\n",
    "2. Выполнить создание таблиц, выполнив скрипт create_tables.sql\n",
    "3. Далее выполняем загрузку в таблицы GA_Sessions и GA_Hits первоначальной базы из CSV-файлов. Заказчиком были предоставлены следующие файлы:\n",
    " - ga_sessions.csv;\n",
    " - ga_hits.csv.\n",
    " Файлы отсутствуют в проекте в связи с тем, что занимают много места. Структура их совпадает со структурой таблиц, поэтому загрузку необходимо проивести стандартными средствами, через \"импорт таблицы\". Для проверки проекта можно и не загружать эти файлы, а загружать данные из JSON-файлов в пустые таблицы.\n",
    "4. Создаем индексы и ключи по таблицам. Целесообразнее это делать после загрузки первоначальных данных. Для создания индексов и ключей используем скрипт Make_Index.sql.\n",
    "\n",
    "### Для операционной системы Windows необходимо установить Git Bash\n",
    "Скачать модуль можно по адресу:  [Git Bash](https://gitforwindows.org/)\n",
    "Послу скачивания необходимо запустить инсталлятор и отвечать положительно на все его запросы. По окончании процедуры для использования утилиты необходимо запустить интерпретатор команднйо строки:\n",
    "![](https://content.codecademy.com/courses/freelance-1/unit-3/git%20bash%20setup/annotated_gitbash_start.png)\n",
    "\n",
    "\n",
    "### Устанавливаем AitFlow.\n",
    "Установку делаем на базе докера\n",
    "#### Полезные ссылки\n",
    "-  [Как установить Docker на Window](https://docs.google.com/document/d/1yLN5i_PHEhWHz3dfQ2Vy6hEMQQ4UHtuKPv_0IVfqtqw/edit#heading=h.1cug9uepbjq1)\n",
    "-  [Как установить Docker на macOS](https://docs.docker.com/desktop/install/mac-install)\n",
    "-  [Как установить Docker на Linux](https://docs.docker.com/desktop/install/linux-install)\n",
    "#### Установка AirFlow  с помощью Docker Compose на Windows\n",
    "\n",
    "Airflow — это инструмент для построения потоков данных и управления ими. Он используется для автоматизации выполнения рутинных задач. \n",
    "\n",
    "В этой инструкции мы рассмотрим, как установить Airflow на компьютер с операционной системой Windows. Перед установкой убедитесь, что у вас — подсистема Windows для Linux (WSL), содержащая Docker Compose. \n",
    "\n",
    "#### Почему используем Docker Compose\n",
    "\n",
    "Docker Compose — это инструмент для описания и запуска многоконтейнерных приложений Docker. Его часто используют для установки таких приложений, как Apache Airflow, так как он позволяет просто управлять приложением: масштабировать его вместе с зависимостями, например, с базами данных и брокерами сообщений.\n",
    "\n",
    "Использование Docker Compose для установки Apache Airflow в Windows имеет несколько преимуществ:\n",
    "\n",
    "- Позволяет запускать Airflow в согласованной и воспроизводимой среде независимо от операционной системы хоста или системных зависимостей.\n",
    "- Упрощает установку и обновление Airflow, а также любых других зависимостей, таких как база метаданных или брокер сообщений.\n",
    "- Позволяет запускать Airflow и его зависимости в отдельных контейнерах, которые могут масштабироваться и управляться независимо.\n",
    "\n",
    "В целом, использование Docker Compose для установки Apache Airflow в Windows может сэкономить ваше время и усилия, позволяя использовать простой метод управления приложением и его зависимостями.\n",
    "\n",
    "##### Шаг 1\n",
    "Создайте папку для Airflow в корне пользовательской папки (путь ~/):\n",
    "cd ~\n",
    "mkdir airflow-docker\n",
    "cd airflow-docker\n",
    "\n",
    "##### Шаг 2\n",
    "Подготовьте окружение — создайте папки, в которых будут храниться даги, логи и плагины:\n",
    "mkdir ./dags ./logs ./plugins\n",
    "\n",
    "Важно учесть, что в Linux смонтированные тома в контейнере используют собственные разрешения файловой системы Linux, поэтому контейнер и хост должны иметь одинаковые права доступа к файлам:\n",
    "echo -e \"AIRFLOW_UID=$(id -u)\\nAIRFLOW_GID=0\" > .env\n",
    "\n",
    "##### Шаг 3\n",
    "\n",
    "Окружение подготовлено! Теперь поднимите airflow-init с помощью Docker Сompose — этот контейнер выполнит инициализацию базы данных, а также зарегистрирует первую учётную запись пользователя:\n",
    "\n",
    "docker-compose up airflow-init\n",
    "\n",
    "Когда база проинициализирована и пользователь создан, можно поднимать Docker Compose для Airflow:\n",
    "\n",
    "docker-compose up\n",
    "\n",
    "Отлично! Теперь можете зайти в веб-интерфейс Airflow по адресу localhost:8080, ввести стандартные логин и пароль (airflow/airflow) и убедиться, что всё работает.\n",
    "\n",
    "#### Подсказки по установке работе\n",
    "\n",
    "В этом блоке вы рассмотрите моменты, которые могут вызвать сложности при выполнении практической работы к модулю.\n",
    "\n",
    "##### Подсказка 1. Обязательно скопируйте даг из папки со скриптами практической работы в папку для дагов, которую вы создали в шаге 2:\n",
    "\n",
    "cp ~/airflow_hw/dags/hw_dag.py ~/airflow-docker/dags\n",
    "\n",
    "##### Подсказка 2. Когда скрипт получения предсказаний готов и отлажен локально, вам нужно положить его и скрипт пайплайна в контейнер worker, чтобы они были видны для Airflow. Ещё их нужно положить в контейнер scheduler, иначе веб-интерфейс будет показывать ошибку при импорте дага.\n",
    "\n",
    "Узнайте ID контейнеров worker и scheduler:\n",
    "\n",
    "docker ps | grep worker\n",
    "docker ps | grep scheduler\n",
    "\n",
    "Скопируйте исполняемый код и данные в контейнеры, указав вместо worker_id и scheduler_id идентификационные номера ваших контейнеров:\n",
    "\n",
    "docker cp ~/airflow_hw worker_id:/home/airflow/airflow_hw\n",
    "docker cp ~/airflow_hw scheduler_id:/home/airflow/airflow_hw\n",
    "\n",
    "Напоминаю, что ~/ — это обозначение для корня пользовательской папки. Если вы работали со скриптами прямо в WSL, то менять путь не нужно. Если вы работали с файлами в Windows, то вам нужно указать абсолютный путь к файлам в хранилище Windows. Диск C доступен по адресу /mnt/c/, проследуйте от него к директории с проектом. Например, если директория с проектом лежит в пользовательской папке Windows, то путь будет таким:\n",
    "\n",
    "/mnt/c/Users/%username%/airflow_hw\n",
    "\n",
    "После команды docker cp сначала укажите, откуда скопировать, а затем — куда скопировать (копируйте в /home/airflow/airflow_hw).\n",
    "\n",
    "Далее зайдите в командную строку контейнеров и установите нужные для работы пайплайна/предикта пакеты (например, scikit-learn):\n",
    "\n",
    "docker exec -it worker_id bash\n",
    "pip install scikit-learn\n",
    "\n",
    "docker exec -it scheduler_id bash\n",
    "pip install scikit-learn\n",
    "\n",
    "\n",
    "##### Подсказка 3. Иногда при запуске пайплайна может возникать ошибка при сохранении pkl-дампа модели. Такое бывает при копировании папки проекта с Windows. Чтобы исправить это поведение, нужно зайти в контейнеры worker и scheduler от имени root-пользователя и дать права командой chmod:\n",
    "\n",
    "docker exec -it -u root worker_id bash\n",
    "cd /home/airflow/airflow_hw\n",
    "chmod -R 777 data dags modules\n",
    "\n",
    "docker exec -it -u root scheduler_id bash\n",
    "cd /home/airflow/airflow_hw\n",
    "chmod -R 777 data dags modules\n",
    "\n",
    "### Прием JSON-файлов\n",
    "Для обработки файлов, т.е. для их загрузки в таблицы, необходимо файлы формата JSON положить по пути: \n",
    " worker_id:/home/airflow/airflow_hw/data/jsons командами:\n",
    " docker cp <место раположения файлов на локальном компьютере> worker_id:/home/airflow/airflow_hw/data/jsons\n",
    "docker cp <место раположения файлов на локальном компьютере>w scheduler_id:/home/airflow/airflow_hw/data/jsons,\n",
    "где индексы worker_id: и scheduler_id: можно узнать выполнив команды:\n",
    "\n",
    "docker ps | grep worker\n",
    "\n",
    "docker ps | grep scheduler\n",
    "\n",
    "Данные команды должны выполняться в командном интерпретаторе Git Bash, если у Вас операционная система на базе Windows.\n",
    "После обработкой JSON-файлы переносятся в каталог \n",
    "  id:/home/airflow/airflow_hw/data/jsons/arch соответственно.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
